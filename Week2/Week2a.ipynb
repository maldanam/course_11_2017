{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0fff099-32b4-4d5f-94f2-dc957e8d7b58",
    "_uuid": "0a7336426503aa654b923d813483e106306716f9"
   },
   "source": [
    "# Week 1 - The What Why and when of Machine Learning\n",
    "Main idea is to follow that notebook in order to have an initial aproach -  this doesn't means that you only have to answer to the questions or implement code. Feel free to add all the interesting content that you find and take conclusions from the data. Please just use this as a template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library import and list input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a4c3c51c-f465-4cfc-913a-ee1225e57724",
    "_uuid": "15592da27c5f06e6ab4fb0a293a851cc8489453b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "#Ignore Warnings - save some confusion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Pandas more columns\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Add input as import path\n",
    "sys.path.insert(0,'../input')\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Import the data from the dataset\n",
    "train_data = pd.read_csv('../input/train.csv',index_col='id')\n",
    "test_data = pd.read_csv('../input/test.csv',index_col='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d96be262-1ded-4cd3-9193-03d7dc0b86bb",
    "_uuid": "823783aab011b613db23022d071be8684faa60e0"
   },
   "source": [
    "# 1. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "### 1.1 Data engineering.\n",
    "Objectives:\n",
    "* Create three new features using the actual features.\n",
    "* Create plots like in the first session. (Categorical / Numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Mrs', ' Miss', ' Mr', ' Master', ' Jonkheer', ' Dr', ' Mme',\n",
       "       ' Ms', ' Major', ' Rev', ' the Countess', ' Dona'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo 1 : Create a new feature that describes if people travels alone or not.\n",
    "#dataset['TravelAlone'] = ...\n",
    "\n",
    "# Todo 2 : Create a new feature that represents the FamilySize - for each passenger a new column with a number 1..n\n",
    "#dataset['FamilySize'] = ...\n",
    "\n",
    "# Todo 3 : Create a new feature based in the Title name for example Sr, Mrs, Rev.\n",
    "# Clue 1: As we can see different classes of title name exist. Decide how to group and decide \n",
    "train_data['name'].str.split(\",\",expand=True)[1].str.split(\".\",expand=True)[0].unique()\n",
    "# Your work\n",
    "dataset['RareName'] = ....\n",
    "\n",
    "# Optional : Plot the data in plots like in the previous training notebook (Categorical / Numerical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "### 1.2 Data correlation.\n",
    "Objectives:\n",
    "* Create a heatmap that show the data correlation\n",
    "* Understand the data correlation\n",
    "\n",
    "Questions:\n",
    "* Q1: Which are the top three pairs of data with more correlation? (ToDo 1..2 and 3..4 ) \n",
    "* Q2: Is the sex of the passengers related with survival?\n",
    "* Q3: If two features have a lot of correlation we want to preserve both or only use one?\n",
    "* Q4: Are the previous new created features related with the survivance in the accident? Choose the one that you think that is the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "ac3cbd95-2242-4251-90ca-dd65c34e17b3",
    "_uuid": "6745659346d0b3d3a9b6424ce123307f20be533f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pclass       age\n",
      "pclass  1.000000 -0.417239\n",
      "age    -0.417239  1.000000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-576609c850ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# PreTodo. Copying train_data into another variable. Some examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Dropping target variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msex_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"male\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"female\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sex\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sex\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msex_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Converting into numerical label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "print (train_data[[\"pclass\",\"age\"]].corr())\n",
    "\n",
    "# PreTodo. Copying train_data into another variable. Some examples\n",
    "sample_data = train_data.copy()\n",
    "sample_data.drop(columns=[\"body\"], inplace=True) #Dropping target variable\n",
    "sex_types = {\"male\":1, \"female\":0}\n",
    "sample_data[\"sex\"] = sample_data[\"sex\"].apply(sex_types.get) # Converting into numerical label\n",
    "#Work with your train data , the same way we work with sample_data\n",
    "\n",
    "\n",
    "\n",
    "# ToDO 1: Compute the correlation for all the features ( Remember that the survived column is our label )\n",
    "print(\"Data Correlation for the features pclass/age\")\n",
    "data_correlation = train_data.corr()\n",
    "# ToDo 2: Plot the data in a fancy plot like the sns.heatmap\n",
    "sns.heatmap(....)\n",
    "# ToDo 3: Compute the correlation between the sex feature and survived label \n",
    "#sex_survived_correlation = ...\n",
    "# ToDo 4: Plot the data in a fancy plot like the sns.heatmap\n",
    "#sns.heatmap(....)\n",
    "# Todo 3: Questions\n",
    "print(\"Q1: This is my response for the first question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "# 2. Data visualization.\n",
    "Objectives:\n",
    "* Use PCA to reduce the dimensions of our data\n",
    "* Analyze the separability of our data\n",
    "\n",
    "Questions:\n",
    "* Q1: Is the data separable?\n",
    "* Q2: You think that we need to reduce the dimensionality of our data?\n",
    "* Q3: Find whitch infromation returns the explained_variance_ratio_ method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "# \n",
    "\"\"\"\n",
    "First of all we need to scale our data we have different features measured in different \"units\" what we need is to normali\n",
    "this features to be in the same range, the easiest way to do this is with the MinMaxScaler\n",
    "\"\"\"\n",
    "\n",
    "# 1. Select a subset of data\n",
    "#train_data_subset = train_data[[\"age\", \"sex\", \"pclass\", \"embarked\", \"TravelAlone\"]]\n",
    "train_data_subset = train_data[[\"age\", \"pclass\", \"sibsp\", \"embarked\", \"sex\"]]\n",
    "\n",
    "# Todo 1: Convert the categorical labels to numerical for example Sex can be converted to a boolean\n",
    "train_data_subset[\"sex\"] = ...\n",
    "train_data_subset[\"embarked\"] = ....\n",
    "\n",
    "# 2. Remove NA with 0 probably exist a better way :)\n",
    "train_data_subset.fillna(0, inplace=True)\n",
    "\n",
    "# 3. Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data_subset), columns=train_data_subset.columns)\n",
    "\n",
    "#After\n",
    "print(\"Before scale:\")\n",
    "print(train_data_subset[0:3])\n",
    "print(\"\\n\")\n",
    "#Before\n",
    "print(\"After scale:\")\n",
    "print(train_data_scaled[0:3])\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Compute PCA\n",
    "pca = sklearnPCA(n_components=2) #2-dimensional PCA\n",
    "transformed = pd.DataFrame(pca.fit_transform(train_data_scaled))\n",
    "train_data.index = range(len(train_data.index))\n",
    "\n",
    "# 5. Plot the data with the reduced dimentions\n",
    "plt.scatter(transformed[train_data[\"survived\"]==0][0], transformed[train_data[\"survived\"]==0][1], label='Died', c='red')\n",
    "plt.scatter(transformed[train_data[\"survived\"]==1][1], transformed[train_data[\"survived\"]==1][1], label='Survived', c='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6. Interesting methods\n",
    "print(pca.explained_variance_ratio_ )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
